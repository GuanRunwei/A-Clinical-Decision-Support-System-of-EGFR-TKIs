{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "18fd1df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import recall_score\n",
    "from joblib import dump, load\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import joblib\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "61153ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "d78f6505",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "setup_seed(666)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "24b9e528",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_file = 'out_validation.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "5b3cf2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'train_and_test.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "4a3469da",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataframe = pd.read_excel(file, sheet_name=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "668eca7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataframe1 = pd.read_excel(new_file, sheet_name=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "3772df7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 52 entries, 0 to 51\n",
      "Data columns (total 16 columns):\n",
      " #   Column  Non-Null Count  Dtype\n",
      "---  ------  --------------  -----\n",
      " 0   性别      52 non-null     int64\n",
      " 1   年龄      52 non-null     int64\n",
      " 2   吸烟      52 non-null     int64\n",
      " 3   原发灶大小   52 non-null     int64\n",
      " 4   骨转移     52 non-null     int64\n",
      " 5   脑转移     52 non-null     int64\n",
      " 6   肝转      52 non-null     int64\n",
      " 7   胸膜转移    52 non-null     int64\n",
      " 8   联合治疗    52 non-null     int64\n",
      " 9   转移个数    52 non-null     int64\n",
      " 10  位点      52 non-null     int64\n",
      " 11  TP53    52 non-null     int64\n",
      " 12  PIK3CA  52 non-null     int64\n",
      " 13  RB1     52 non-null     int64\n",
      " 14  TMB     52 non-null     int64\n",
      " 15  END     52 non-null     int64\n",
      "dtypes: int64(16)\n",
      "memory usage: 6.6 KB\n"
     ]
    }
   ],
   "source": [
    "new_dataframe1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "16c7bde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(y_test, y_pred):\n",
    "    zero_length = len([i for i in y_test if i == 0])\n",
    "    print(\"zero_length\", zero_length)\n",
    "#     print(y_test)\n",
    "#     print(y_pred)\n",
    "    zero_true_count = 0\n",
    "    for i in range(len(y_test)):\n",
    "        if y_test[i] == 0 and y_pred[i] == 0:\n",
    "            zero_true_count += 1\n",
    "    return zero_true_count / zero_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "610fac33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset2\n",
    "X_second = new_dataframe1.iloc[:,range(0,15)]\n",
    "y_second = new_dataframe1.iloc[:, -1]  # 第二个指标\n",
    "X_second = np.asarray(X_second)\n",
    "X_second = torch.Tensor(X_second)\n",
    "y_second = torch.Tensor(y_second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "a4917d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset2\n",
    "X_second1 = new_dataframe1.iloc[:,range(0,15)]\n",
    "y_second1 = new_dataframe1.iloc[:, -1]  # 第二个指标\n",
    "X_second1 = np.asarray(X_second1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "9966e04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_second1 = np.array(y_second1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "a63276b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_second, X_test_second, y_train_second, y_test_second = train_test_split(X_second1, y_second1, test_size=0.20, random_state=100)\n",
    "X_train_second = torch.Tensor(X_train_second)\n",
    "X_test_second = torch.Tensor(X_test_second)\n",
    "y_train_second = torch.Tensor(y_train_second)\n",
    "y_test_second = torch.Tensor(y_test_second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "5c9364bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feedforward_second(torch.nn.Module):\n",
    "        def __init__(self, input_size, hidden_size):\n",
    "            super(Feedforward_second, self).__init__()\n",
    "            self.input_size = input_size\n",
    "            self.hidden_size  = hidden_size\n",
    "            self.batchnorm = torch.nn.BatchNorm1d(self.hidden_size)\n",
    "            self.relu = torch.nn.ReLU()\n",
    "            self.fc1 = torch.nn.Linear(self.input_size, self.hidden_size, bias=True)\n",
    "            self.dropout1 = torch.nn.Dropout(p=0.05)\n",
    "            self.relu1 = torch.nn.ReLU()\n",
    "            self.fc2 = torch.nn.Linear(self.hidden_size, 16, bias=True)\n",
    "            self.dropout2 = torch.nn.Dropout(p=0.05)\n",
    "            self.relu2 = torch.nn.ReLU()\n",
    "            self.fc3 = torch.nn.Linear(16, 1, bias=True)\n",
    "            self.sigmoid = torch.nn.Sigmoid()\n",
    "        def forward(self, x):\n",
    "            hidden = self.fc1(x)\n",
    "#             batchnorm = self.batchnorm(hidden)\n",
    "            hidden = self.relu(hidden)\n",
    "            relu = self.relu1(hidden)\n",
    "            output = self.fc2(relu)\n",
    "            output = self.relu2(output)\n",
    "            output = self.fc3(output)\n",
    "            output = self.sigmoid(output)\n",
    "            return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "162666a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Feedforward_second(X_train_second.shape[1],20)\n",
    "criterion2 = torch.nn.BCELoss()\n",
    "optimizer2 = torch.optim.AdamW(model2.parameters(), lr = 0.005, weight_decay= 5e-4)\n",
    "scheduler2 = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer2, gamma=0.92)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "0e95447f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train loss: 0.7061502933502197\n",
      "Epoch 1: train loss: 0.699498176574707\n",
      "Epoch 2: train loss: 0.6953036189079285\n",
      "Epoch 3: train loss: 0.6927807927131653\n",
      "Epoch 4: train loss: 0.6905691623687744\n",
      "Epoch 5: train loss: 0.6888396739959717\n",
      "Epoch 6: train loss: 0.6875101923942566\n",
      "Epoch 7: train loss: 0.6863274574279785\n",
      "Epoch 8: train loss: 0.6851624250411987\n",
      "Epoch 9: train loss: 0.6838745474815369\n",
      "Epoch 10: train loss: 0.6824457049369812\n",
      "Epoch 11: train loss: 0.680958092212677\n",
      "Epoch 12: train loss: 0.6793220043182373\n",
      "Epoch 13: train loss: 0.6773959994316101\n",
      "Epoch 14: train loss: 0.6752957105636597\n",
      "Epoch 15: train loss: 0.6728329062461853\n",
      "Epoch 16: train loss: 0.6702690720558167\n",
      "Epoch 17: train loss: 0.6675310134887695\n",
      "Epoch 18: train loss: 0.6643450260162354\n",
      "Epoch 19: train loss: 0.6607840657234192\n",
      "Epoch 20: train loss: 0.6566256880760193\n",
      "Epoch 21: train loss: 0.6519157290458679\n",
      "Epoch 22: train loss: 0.6465938687324524\n",
      "Epoch 23: train loss: 0.641169548034668\n",
      "Epoch 24: train loss: 0.6354710459709167\n",
      "Epoch 25: train loss: 0.6293408274650574\n",
      "Epoch 26: train loss: 0.6225815415382385\n",
      "Epoch 27: train loss: 0.6154012680053711\n",
      "Epoch 28: train loss: 0.6080647706985474\n",
      "Epoch 29: train loss: 0.6004346013069153\n",
      "Epoch 30: train loss: 0.5922048687934875\n",
      "Epoch 31: train loss: 0.5836682915687561\n",
      "Epoch 32: train loss: 0.5746481418609619\n",
      "Epoch 33: train loss: 0.5653460621833801\n",
      "Epoch 34: train loss: 0.5558675527572632\n",
      "Epoch 35: train loss: 0.546230673789978\n",
      "Epoch 36: train loss: 0.5365698933601379\n",
      "Epoch 37: train loss: 0.5269073247909546\n",
      "Epoch 38: train loss: 0.5173790454864502\n",
      "Epoch 39: train loss: 0.5080088376998901\n",
      "Epoch 40: train loss: 0.498980313539505\n",
      "Epoch 41: train loss: 0.49010178446769714\n",
      "Epoch 42: train loss: 0.4817298650741577\n",
      "Epoch 43: train loss: 0.4736539423465729\n",
      "Epoch 44: train loss: 0.4659731388092041\n",
      "Epoch 45: train loss: 0.4586063027381897\n",
      "Epoch 46: train loss: 0.4516736567020416\n",
      "Epoch 47: train loss: 0.4453081786632538\n",
      "Epoch 48: train loss: 0.4392088055610657\n",
      "Epoch 49: train loss: 0.4334118068218231\n",
      "Epoch 50: train loss: 0.4279232919216156\n",
      "Epoch 51: train loss: 0.42294976115226746\n",
      "Epoch 52: train loss: 0.418087363243103\n",
      "Epoch 53: train loss: 0.41335272789001465\n",
      "Epoch 54: train loss: 0.4089620113372803\n",
      "Epoch 55: train loss: 0.40456515550613403\n",
      "Epoch 56: train loss: 0.40027081966400146\n",
      "Epoch 57: train loss: 0.3960506021976471\n",
      "Epoch 58: train loss: 0.3918594717979431\n",
      "Epoch 59: train loss: 0.38773682713508606\n",
      "Epoch 60: train loss: 0.38372689485549927\n",
      "Epoch 61: train loss: 0.37970632314682007\n",
      "Epoch 62: train loss: 0.3758731186389923\n",
      "Epoch 63: train loss: 0.37212079763412476\n",
      "Epoch 64: train loss: 0.3687392771244049\n",
      "Epoch 65: train loss: 0.36485540866851807\n",
      "Epoch 66: train loss: 0.3613949716091156\n",
      "Epoch 67: train loss: 0.3579472005367279\n",
      "Epoch 68: train loss: 0.35463619232177734\n",
      "Epoch 69: train loss: 0.3512503206729889\n",
      "Epoch 70: train loss: 0.347774863243103\n",
      "Epoch 71: train loss: 0.34434616565704346\n",
      "Epoch 72: train loss: 0.341288298368454\n",
      "Epoch 73: train loss: 0.3376040458679199\n",
      "Epoch 74: train loss: 0.33434247970581055\n",
      "Epoch 75: train loss: 0.3310168981552124\n",
      "Epoch 76: train loss: 0.3277257978916168\n",
      "Epoch 77: train loss: 0.3242453336715698\n",
      "Epoch 78: train loss: 0.3209582567214966\n",
      "Epoch 79: train loss: 0.31779563426971436\n",
      "Epoch 80: train loss: 0.3141329288482666\n",
      "Epoch 81: train loss: 0.3106640577316284\n",
      "Epoch 82: train loss: 0.30742600560188293\n",
      "Epoch 83: train loss: 0.30380159616470337\n",
      "Epoch 84: train loss: 0.3003002107143402\n",
      "Epoch 85: train loss: 0.2966687083244324\n",
      "Epoch 86: train loss: 0.29322290420532227\n",
      "Epoch 87: train loss: 0.2898411750793457\n",
      "Epoch 88: train loss: 0.2865060269832611\n",
      "Epoch 89: train loss: 0.283174067735672\n",
      "Epoch 90: train loss: 0.27983126044273376\n",
      "Epoch 91: train loss: 0.27625924348831177\n",
      "Epoch 92: train loss: 0.2728763222694397\n",
      "Epoch 93: train loss: 0.26959118247032166\n",
      "Epoch 94: train loss: 0.26613354682922363\n",
      "Epoch 95: train loss: 0.26338571310043335\n",
      "Epoch 96: train loss: 0.2592495381832123\n",
      "Epoch 97: train loss: 0.25609537959098816\n",
      "Epoch 98: train loss: 0.2530547082424164\n",
      "Epoch 99: train loss: 0.24979551136493683\n",
      "Epoch 100: train loss: 0.24677430093288422\n",
      "Epoch 101: train loss: 0.2434372752904892\n",
      "Epoch 102: train loss: 0.24020828306674957\n",
      "Epoch 103: train loss: 0.2367919385433197\n",
      "Epoch 104: train loss: 0.23401020467281342\n",
      "Epoch 105: train loss: 0.23009344935417175\n",
      "Epoch 106: train loss: 0.22779273986816406\n",
      "Epoch 107: train loss: 0.22437834739685059\n",
      "Epoch 108: train loss: 0.22110499441623688\n",
      "Epoch 109: train loss: 0.2182525098323822\n",
      "Epoch 110: train loss: 0.21594126522541046\n",
      "Epoch 111: train loss: 0.2134944349527359\n",
      "Epoch 112: train loss: 0.21048545837402344\n",
      "Epoch 113: train loss: 0.2067093551158905\n",
      "Epoch 114: train loss: 0.20405015349388123\n",
      "Epoch 115: train loss: 0.2018975466489792\n",
      "Epoch 116: train loss: 0.19898328185081482\n",
      "Epoch 117: train loss: 0.19495858252048492\n",
      "Epoch 118: train loss: 0.19299176335334778\n",
      "Epoch 119: train loss: 0.19049037992954254\n",
      "Epoch 120: train loss: 0.1867029219865799\n",
      "Epoch 121: train loss: 0.18462562561035156\n",
      "Epoch 122: train loss: 0.18135440349578857\n",
      "Epoch 123: train loss: 0.17912501096725464\n",
      "Epoch 124: train loss: 0.1767929643392563\n",
      "Epoch 125: train loss: 0.17431606352329254\n",
      "Epoch 126: train loss: 0.17169924080371857\n",
      "Epoch 127: train loss: 0.1688780039548874\n",
      "Epoch 128: train loss: 0.16587506234645844\n",
      "Epoch 129: train loss: 0.16304321587085724\n",
      "Epoch 130: train loss: 0.16148246824741364\n",
      "Epoch 131: train loss: 0.15829189121723175\n",
      "Epoch 132: train loss: 0.15586808323860168\n",
      "Epoch 133: train loss: 0.1539096236228943\n",
      "Epoch 134: train loss: 0.15209494531154633\n",
      "Epoch 135: train loss: 0.14944308996200562\n",
      "Epoch 136: train loss: 0.14681214094161987\n",
      "Epoch 137: train loss: 0.14422518014907837\n",
      "Epoch 138: train loss: 0.14154992997646332\n",
      "Epoch 139: train loss: 0.13917265832424164\n",
      "Epoch 140: train loss: 0.13657574355602264\n",
      "Epoch 141: train loss: 0.13398292660713196\n",
      "Epoch 142: train loss: 0.13181307911872864\n",
      "Epoch 143: train loss: 0.12950292229652405\n",
      "Epoch 144: train loss: 0.12723758816719055\n",
      "Epoch 145: train loss: 0.12481965124607086\n",
      "Epoch 146: train loss: 0.12240651994943619\n",
      "Epoch 147: train loss: 0.12018553912639618\n",
      "Epoch 148: train loss: 0.11793769150972366\n",
      "Epoch 149: train loss: 0.11609579622745514\n",
      "Epoch 150: train loss: 0.11416041105985641\n",
      "Epoch 151: train loss: 0.11200952529907227\n",
      "Epoch 152: train loss: 0.11067532002925873\n",
      "Epoch 153: train loss: 0.10862322896718979\n",
      "Epoch 154: train loss: 0.10567120462656021\n",
      "Epoch 155: train loss: 0.10317675769329071\n",
      "Epoch 156: train loss: 0.10216207057237625\n",
      "Epoch 157: train loss: 0.10080204904079437\n",
      "Epoch 158: train loss: 0.09884914755821228\n",
      "Epoch 159: train loss: 0.09598196297883987\n",
      "Epoch 160: train loss: 0.09411494433879852\n",
      "Epoch 161: train loss: 0.09293553978204727\n",
      "Epoch 162: train loss: 0.09113018959760666\n",
      "Epoch 163: train loss: 0.08893893659114838\n",
      "Epoch 164: train loss: 0.08725503832101822\n",
      "Epoch 165: train loss: 0.08645644038915634\n",
      "Epoch 166: train loss: 0.08502095192670822\n",
      "Epoch 167: train loss: 0.0827166885137558\n",
      "Epoch 168: train loss: 0.08103779703378677\n",
      "Epoch 169: train loss: 0.07999415695667267\n",
      "Epoch 170: train loss: 0.07828986644744873\n",
      "Epoch 171: train loss: 0.07644037157297134\n",
      "Epoch 172: train loss: 0.07530058920383453\n",
      "Epoch 173: train loss: 0.0740562230348587\n",
      "Epoch 174: train loss: 0.072503961622715\n",
      "Epoch 175: train loss: 0.07092254608869553\n",
      "Epoch 176: train loss: 0.07002466171979904\n",
      "Epoch 177: train loss: 0.0689438134431839\n",
      "Epoch 178: train loss: 0.06740689277648926\n",
      "Epoch 179: train loss: 0.06626535207033157\n",
      "Epoch 180: train loss: 0.06524550169706345\n",
      "Epoch 181: train loss: 0.06374765932559967\n",
      "Epoch 182: train loss: 0.0624983012676239\n",
      "Epoch 183: train loss: 0.06178294122219086\n",
      "Epoch 184: train loss: 0.06061039865016937\n",
      "Epoch 185: train loss: 0.059368155896663666\n",
      "Epoch 186: train loss: 0.05818064138293266\n",
      "Epoch 187: train loss: 0.05726372450590134\n",
      "Epoch 188: train loss: 0.05625632777810097\n",
      "Epoch 189: train loss: 0.0552077516913414\n",
      "Epoch 190: train loss: 0.05430157482624054\n",
      "Epoch 191: train loss: 0.053409598767757416\n",
      "Epoch 192: train loss: 0.052432581782341\n",
      "Epoch 193: train loss: 0.0514865517616272\n",
      "Epoch 194: train loss: 0.05071878433227539\n",
      "Epoch 195: train loss: 0.04993485286831856\n",
      "Epoch 196: train loss: 0.04896814003586769\n",
      "Epoch 197: train loss: 0.04815835878252983\n",
      "Epoch 198: train loss: 0.04741913452744484\n",
      "Epoch 199: train loss: 0.04667215049266815\n",
      "Epoch 200: train loss: 0.045865729451179504\n",
      "Epoch 201: train loss: 0.04508218169212341\n",
      "Epoch 202: train loss: 0.04432089626789093\n",
      "Epoch 203: train loss: 0.043579503893852234\n",
      "Epoch 204: train loss: 0.04282546415925026\n",
      "Epoch 205: train loss: 0.04226516932249069\n",
      "Epoch 206: train loss: 0.0415993332862854\n",
      "Epoch 207: train loss: 0.04085525870323181\n",
      "Epoch 208: train loss: 0.04019276425242424\n",
      "Epoch 209: train loss: 0.03964772820472717\n",
      "Epoch 210: train loss: 0.03901475667953491\n",
      "Epoch 211: train loss: 0.03833657503128052\n",
      "Epoch 212: train loss: 0.03776773437857628\n",
      "Epoch 213: train loss: 0.03729528188705444\n",
      "Epoch 214: train loss: 0.03669436275959015\n",
      "Epoch 215: train loss: 0.0361127108335495\n",
      "Epoch 216: train loss: 0.035663481801748276\n",
      "Epoch 217: train loss: 0.03509395569562912\n",
      "Epoch 218: train loss: 0.034529317170381546\n",
      "Epoch 219: train loss: 0.03407691419124603\n",
      "Epoch 220: train loss: 0.03357073664665222\n",
      "Epoch 221: train loss: 0.03301828354597092\n",
      "Epoch 222: train loss: 0.032536331564188004\n",
      "Epoch 223: train loss: 0.032101478427648544\n",
      "Epoch 224: train loss: 0.03166261687874794\n",
      "Epoch 225: train loss: 0.031191453337669373\n",
      "Epoch 226: train loss: 0.030741354450583458\n",
      "Epoch 227: train loss: 0.0303342305123806\n",
      "Epoch 228: train loss: 0.029880346730351448\n",
      "Epoch 229: train loss: 0.029477527365088463\n",
      "Epoch 230: train loss: 0.02910388447344303\n",
      "Epoch 231: train loss: 0.028658093884587288\n",
      "Epoch 232: train loss: 0.02825537510216236\n",
      "Epoch 233: train loss: 0.027927415445446968\n",
      "Epoch 234: train loss: 0.027548136189579964\n",
      "Epoch 235: train loss: 0.02717217616736889\n",
      "Epoch 236: train loss: 0.02677246741950512\n",
      "Epoch 237: train loss: 0.026417624205350876\n",
      "Epoch 238: train loss: 0.026078730821609497\n",
      "Epoch 239: train loss: 0.02572590485215187\n",
      "Epoch 240: train loss: 0.025395289063453674\n",
      "Epoch 241: train loss: 0.025075389072299004\n",
      "Epoch 242: train loss: 0.024753963574767113\n",
      "Epoch 243: train loss: 0.024423794820904732\n",
      "Epoch 244: train loss: 0.024112675338983536\n",
      "Epoch 245: train loss: 0.023808471858501434\n",
      "Epoch 246: train loss: 0.023508045822381973\n",
      "Epoch 247: train loss: 0.02321009710431099\n",
      "Epoch 248: train loss: 0.022914092987775803\n",
      "Epoch 249: train loss: 0.022631675004959106\n",
      "Epoch 250: train loss: 0.022363778203725815\n",
      "Epoch 251: train loss: 0.022085048258304596\n",
      "Epoch 252: train loss: 0.021790403872728348\n",
      "Epoch 253: train loss: 0.021545426920056343\n",
      "Epoch 254: train loss: 0.02127254754304886\n",
      "Epoch 255: train loss: 0.021021969616413116\n",
      "Epoch 256: train loss: 0.020775295794010162\n",
      "Epoch 257: train loss: 0.020530518144369125\n",
      "Epoch 258: train loss: 0.02029777504503727\n",
      "Epoch 259: train loss: 0.02005963772535324\n",
      "Epoch 260: train loss: 0.019815312698483467\n",
      "Epoch 261: train loss: 0.019562862813472748\n",
      "Epoch 262: train loss: 0.019341837614774704\n",
      "Epoch 263: train loss: 0.01911812648177147\n",
      "Epoch 264: train loss: 0.01887005940079689\n",
      "Epoch 265: train loss: 0.01868077926337719\n",
      "Epoch 266: train loss: 0.01846066489815712\n",
      "Epoch 267: train loss: 0.01828186959028244\n",
      "Epoch 268: train loss: 0.01806855946779251\n",
      "Epoch 269: train loss: 0.01785729080438614\n",
      "Epoch 270: train loss: 0.017664965242147446\n",
      "Epoch 271: train loss: 0.017452456057071686\n",
      "Epoch 272: train loss: 0.01724701002240181\n",
      "Epoch 273: train loss: 0.01705675944685936\n",
      "Epoch 274: train loss: 0.01686980575323105\n",
      "Epoch 275: train loss: 0.01670726388692856\n",
      "Epoch 276: train loss: 0.016487210988998413\n",
      "Epoch 277: train loss: 0.01631120778620243\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 278: train loss: 0.016149960458278656\n",
      "Epoch 279: train loss: 0.015972083434462547\n",
      "Epoch 280: train loss: 0.01582026667892933\n",
      "Epoch 281: train loss: 0.01564384438097477\n",
      "Epoch 282: train loss: 0.015462471172213554\n",
      "Epoch 283: train loss: 0.015297477133572102\n",
      "Epoch 284: train loss: 0.015128478407859802\n",
      "Epoch 285: train loss: 0.014971606433391571\n",
      "Epoch 286: train loss: 0.014803868718445301\n",
      "Epoch 287: train loss: 0.014660348184406757\n",
      "Epoch 288: train loss: 0.01451551727950573\n",
      "Epoch 289: train loss: 0.01435779221355915\n",
      "Epoch 290: train loss: 0.014206787571310997\n",
      "Epoch 291: train loss: 0.014041617512702942\n",
      "Epoch 292: train loss: 0.013919119723141193\n",
      "Epoch 293: train loss: 0.013779324479401112\n",
      "Epoch 294: train loss: 0.013619761914014816\n",
      "Epoch 295: train loss: 0.013493304140865803\n",
      "Epoch 296: train loss: 0.013348481617867947\n",
      "Epoch 297: train loss: 0.01321993675082922\n",
      "Epoch 298: train loss: 0.013104837387800217\n",
      "Epoch 299: train loss: 0.012966846115887165\n",
      "Epoch 300: train loss: 0.012835871428251266\n",
      "Epoch 301: train loss: 0.012714666314423084\n",
      "Epoch 302: train loss: 0.01257209200412035\n",
      "Epoch 303: train loss: 0.012454836629331112\n",
      "Epoch 304: train loss: 0.012323335744440556\n",
      "Epoch 305: train loss: 0.012198824435472488\n",
      "Epoch 306: train loss: 0.01209820993244648\n",
      "Epoch 307: train loss: 0.011983592063188553\n",
      "Epoch 308: train loss: 0.011853335425257683\n",
      "Epoch 309: train loss: 0.011745767667889595\n",
      "Epoch 310: train loss: 0.011637197807431221\n",
      "Epoch 311: train loss: 0.011519086547195911\n",
      "Epoch 312: train loss: 0.011423317715525627\n",
      "Epoch 313: train loss: 0.01130407489836216\n",
      "Epoch 314: train loss: 0.01121211051940918\n",
      "Epoch 315: train loss: 0.011098401620984077\n",
      "Epoch 316: train loss: 0.010992009192705154\n",
      "Epoch 317: train loss: 0.010896601714193821\n",
      "Epoch 318: train loss: 0.010792143642902374\n",
      "Epoch 319: train loss: 0.01071644015610218\n",
      "Epoch 320: train loss: 0.010601850226521492\n",
      "Epoch 321: train loss: 0.01050520408898592\n",
      "Epoch 322: train loss: 0.010407661087810993\n",
      "Epoch 323: train loss: 0.010310199111700058\n",
      "Epoch 324: train loss: 0.010216032154858112\n",
      "Epoch 325: train loss: 0.010122813284397125\n",
      "Epoch 326: train loss: 0.01004484761506319\n",
      "Epoch 327: train loss: 0.00994371809065342\n",
      "Epoch 328: train loss: 0.00985164288431406\n",
      "Epoch 329: train loss: 0.009760749526321888\n",
      "Epoch 330: train loss: 0.009677281603217125\n",
      "Epoch 331: train loss: 0.009587992914021015\n",
      "Epoch 332: train loss: 0.009503440000116825\n",
      "Epoch 333: train loss: 0.009428850375115871\n",
      "Epoch 334: train loss: 0.00933466013520956\n",
      "Epoch 335: train loss: 0.009256289340555668\n",
      "Epoch 336: train loss: 0.009185590781271458\n",
      "Epoch 337: train loss: 0.009100068360567093\n",
      "Epoch 338: train loss: 0.009028498083353043\n",
      "Epoch 339: train loss: 0.008953364565968513\n",
      "Epoch 340: train loss: 0.008871844969689846\n",
      "Epoch 341: train loss: 0.008812379091978073\n",
      "Epoch 342: train loss: 0.008719277568161488\n",
      "Epoch 343: train loss: 0.00864340364933014\n",
      "Epoch 344: train loss: 0.008570159785449505\n",
      "Epoch 345: train loss: 0.008495815098285675\n",
      "Epoch 346: train loss: 0.008429351262748241\n",
      "Epoch 347: train loss: 0.008366178721189499\n",
      "Epoch 348: train loss: 0.008285783231258392\n",
      "Epoch 349: train loss: 0.008219274692237377\n",
      "Epoch 350: train loss: 0.008155605755746365\n",
      "Epoch 351: train loss: 0.008090702816843987\n",
      "Epoch 352: train loss: 0.008027407340705395\n",
      "Epoch 353: train loss: 0.00796438753604889\n",
      "Epoch 354: train loss: 0.007897898554801941\n",
      "Epoch 355: train loss: 0.007832039147615433\n",
      "Epoch 356: train loss: 0.007766533177345991\n",
      "Epoch 357: train loss: 0.0077143143862485886\n",
      "Epoch 358: train loss: 0.007641986478120089\n",
      "Epoch 359: train loss: 0.007575724273920059\n",
      "Epoch 360: train loss: 0.0075216395780444145\n",
      "Epoch 361: train loss: 0.007474767044186592\n",
      "Epoch 362: train loss: 0.007404680829495192\n",
      "Epoch 363: train loss: 0.007357154507189989\n",
      "Epoch 364: train loss: 0.007292696740478277\n",
      "Epoch 365: train loss: 0.007245149463415146\n",
      "Epoch 366: train loss: 0.007184131070971489\n",
      "Epoch 367: train loss: 0.0071325176395475864\n",
      "Epoch 368: train loss: 0.007071082014590502\n",
      "Epoch 369: train loss: 0.0070286341942846775\n",
      "Epoch 370: train loss: 0.0069693285040557384\n",
      "Epoch 371: train loss: 0.006909588351845741\n",
      "Epoch 372: train loss: 0.006858126260340214\n",
      "Epoch 373: train loss: 0.006802211981266737\n",
      "Epoch 374: train loss: 0.006758438888937235\n",
      "Epoch 375: train loss: 0.0067023299634456635\n",
      "Epoch 376: train loss: 0.006654636934399605\n",
      "Epoch 377: train loss: 0.006599893793463707\n",
      "Epoch 378: train loss: 0.0065519786439836025\n",
      "Epoch 379: train loss: 0.006499175447970629\n",
      "Epoch 380: train loss: 0.0064511788077652454\n",
      "Epoch 381: train loss: 0.006405262742191553\n",
      "Epoch 382: train loss: 0.006360021885484457\n",
      "Epoch 383: train loss: 0.006314206402748823\n",
      "Epoch 384: train loss: 0.006260773632675409\n",
      "Epoch 385: train loss: 0.00622025690972805\n",
      "Epoch 386: train loss: 0.006172955967485905\n",
      "Epoch 387: train loss: 0.006129339803010225\n",
      "Epoch 388: train loss: 0.006082605104893446\n",
      "Epoch 389: train loss: 0.006039297673851252\n",
      "Epoch 390: train loss: 0.005995935294777155\n",
      "Epoch 391: train loss: 0.005953292828053236\n",
      "Epoch 392: train loss: 0.005912334192544222\n",
      "Epoch 393: train loss: 0.005869894288480282\n",
      "Epoch 394: train loss: 0.0058295149356126785\n",
      "Epoch 395: train loss: 0.005786524154245853\n",
      "Epoch 396: train loss: 0.005747986491769552\n",
      "Epoch 397: train loss: 0.0057085128501057625\n",
      "Epoch 398: train loss: 0.005669133737683296\n",
      "Epoch 399: train loss: 0.005628995597362518\n",
      "Epoch 400: train loss: 0.005591022316366434\n",
      "Epoch 401: train loss: 0.005552865564823151\n",
      "Epoch 402: train loss: 0.0055144173093140125\n",
      "Epoch 403: train loss: 0.005476427264511585\n",
      "Epoch 404: train loss: 0.005440549459308386\n",
      "Epoch 405: train loss: 0.005399333778768778\n",
      "Epoch 406: train loss: 0.005365691613405943\n",
      "Epoch 407: train loss: 0.005327028688043356\n",
      "Epoch 408: train loss: 0.005291434936225414\n",
      "Epoch 409: train loss: 0.005255649797618389\n",
      "Epoch 410: train loss: 0.005221827886998653\n",
      "Epoch 411: train loss: 0.005187304224818945\n",
      "Epoch 412: train loss: 0.005150545854121447\n",
      "Epoch 413: train loss: 0.005125349387526512\n",
      "Epoch 414: train loss: 0.0050867474637925625\n",
      "Epoch 415: train loss: 0.005052860360592604\n",
      "Epoch 416: train loss: 0.005020460579544306\n",
      "Epoch 417: train loss: 0.004986319690942764\n",
      "Epoch 418: train loss: 0.004951766226440668\n",
      "Epoch 419: train loss: 0.004922684282064438\n",
      "Epoch 420: train loss: 0.004889240488409996\n",
      "Epoch 421: train loss: 0.004856246057897806\n",
      "Epoch 422: train loss: 0.004826222080737352\n",
      "Epoch 423: train loss: 0.00479585537686944\n",
      "Epoch 424: train loss: 0.004765640944242477\n",
      "Epoch 425: train loss: 0.0047347587533295155\n",
      "Epoch 426: train loss: 0.004705151543021202\n",
      "Epoch 427: train loss: 0.004675178322941065\n",
      "Epoch 428: train loss: 0.004643324762582779\n",
      "Epoch 429: train loss: 0.004615635611116886\n",
      "Epoch 430: train loss: 0.00458614993840456\n",
      "Epoch 431: train loss: 0.004556773696094751\n",
      "Epoch 432: train loss: 0.004528421442955732\n",
      "Epoch 433: train loss: 0.0045013404451310635\n",
      "Epoch 434: train loss: 0.004470366053283215\n",
      "Epoch 435: train loss: 0.004442513920366764\n",
      "Epoch 436: train loss: 0.004419751465320587\n",
      "Epoch 437: train loss: 0.004390109796077013\n",
      "Epoch 438: train loss: 0.004362566862255335\n",
      "Epoch 439: train loss: 0.004337191116064787\n",
      "Epoch 440: train loss: 0.004309288691729307\n",
      "Epoch 441: train loss: 0.004282606765627861\n",
      "Epoch 442: train loss: 0.004257959313690662\n",
      "Epoch 443: train loss: 0.0042303502559661865\n",
      "Epoch 444: train loss: 0.004205048084259033\n",
      "Epoch 445: train loss: 0.004182324279099703\n",
      "Epoch 446: train loss: 0.004157500807195902\n",
      "Epoch 447: train loss: 0.004131860099732876\n",
      "Epoch 448: train loss: 0.004107963293790817\n",
      "Epoch 449: train loss: 0.0040814923122525215\n",
      "Epoch 450: train loss: 0.004056720994412899\n",
      "Epoch 451: train loss: 0.004030290059745312\n",
      "Epoch 452: train loss: 0.004011586774140596\n",
      "Epoch 453: train loss: 0.003983126021921635\n",
      "Epoch 454: train loss: 0.003959615249186754\n",
      "Epoch 455: train loss: 0.003936645574867725\n",
      "Epoch 456: train loss: 0.003914217930287123\n",
      "Epoch 457: train loss: 0.003892763052135706\n",
      "Epoch 458: train loss: 0.003870104206725955\n",
      "Epoch 459: train loss: 0.0038483694661408663\n",
      "Epoch 460: train loss: 0.003826110390946269\n",
      "Epoch 461: train loss: 0.003803285537287593\n",
      "Epoch 462: train loss: 0.003780293045565486\n",
      "Epoch 463: train loss: 0.003757743164896965\n",
      "Epoch 464: train loss: 0.0037367576733231544\n",
      "Epoch 465: train loss: 0.00371746439486742\n",
      "Epoch 466: train loss: 0.003694612067192793\n",
      "Epoch 467: train loss: 0.0036757763009518385\n",
      "Epoch 468: train loss: 0.0036550844088196754\n",
      "Epoch 469: train loss: 0.003635496599599719\n",
      "Epoch 470: train loss: 0.0036135725677013397\n",
      "Epoch 471: train loss: 0.0035950082819908857\n",
      "Epoch 472: train loss: 0.0035735720302909613\n",
      "Epoch 473: train loss: 0.003553924849256873\n",
      "Epoch 474: train loss: 0.003532784292474389\n",
      "Epoch 475: train loss: 0.003514755517244339\n",
      "Epoch 476: train loss: 0.0034932417329400778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 477: train loss: 0.003474744036793709\n",
      "Epoch 478: train loss: 0.0034543885849416256\n",
      "Epoch 479: train loss: 0.0034394424874335527\n",
      "Epoch 480: train loss: 0.0034164898097515106\n",
      "Epoch 481: train loss: 0.003398740431293845\n",
      "Epoch 482: train loss: 0.0033786639105528593\n",
      "Epoch 483: train loss: 0.003363225143402815\n",
      "Epoch 484: train loss: 0.0033437979873269796\n",
      "Epoch 485: train loss: 0.0033264472149312496\n",
      "Epoch 486: train loss: 0.0033064819872379303\n",
      "Epoch 487: train loss: 0.0032927049323916435\n",
      "Epoch 488: train loss: 0.0032714104745537043\n",
      "Epoch 489: train loss: 0.0032553577329963446\n",
      "Epoch 490: train loss: 0.003235970623791218\n",
      "Epoch 491: train loss: 0.0032210492063313723\n",
      "Epoch 492: train loss: 0.003201075131073594\n",
      "Epoch 493: train loss: 0.003184799337759614\n",
      "Epoch 494: train loss: 0.003170676063746214\n",
      "Epoch 495: train loss: 0.0031531504355371\n",
      "Epoch 496: train loss: 0.0031348634511232376\n",
      "Epoch 497: train loss: 0.003118664026260376\n",
      "Epoch 498: train loss: 0.00310256564989686\n",
      "Epoch 499: train loss: 0.0030860458500683308\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "now = str(datetime.datetime.now().year) + \"_\" + str(datetime.datetime.now().month) + \"_\" + str(datetime.datetime.now().day) + \"_\" + \\\n",
    "str(datetime.datetime.now().hour) + \"_\" + str(datetime.datetime.now().minute) + \"_\" + str(datetime.datetime.now().second)\n",
    "model2.train()\n",
    "epoch = 500\n",
    "loss_array = []\n",
    "highest_accuracy = 0\n",
    "for epoch in range(epoch):\n",
    "    optimizer2.zero_grad()\n",
    "    # Forward pass\n",
    "    y_pred = model2(X_train_second)\n",
    "    # Compute Loss\n",
    "    loss = criterion2(y_pred.squeeze(), y_train_second)\n",
    "    loss_array.append(float(loss.item()))\n",
    "    print('Epoch {}: train loss: {}'.format(epoch, loss.item()))\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer2.step()\n",
    "    model2.eval()\n",
    "    y_pred_second = model2(X_test_second)\n",
    "    y_pred_second_int = []\n",
    "    for item in y_pred_second:\n",
    "        y_pred_second_int.append(round(float(item[0])))\n",
    "#     fpr2, tpr2, thersholds2 = roc_curve(y_test_second, y_pred_second_int, pos_label=1)\n",
    "    test_acc = np.sum(y_pred_second_int==np.array(y_test_second))/len(y_test_second)\n",
    "    if test_acc > highest_accuracy:\n",
    "        highest_accuracy = test_acc\n",
    "        torch.save(model2, f\"ann_models/model2_{test_acc}.pt\")\n",
    "\n",
    "\n",
    "#     scheduler2.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "e31bbeef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8181818181818182"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "highest_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "bfcf65b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "internal validation accuracy: 0.8181818181818182\n"
     ]
    }
   ],
   "source": [
    "model2 = torch.load(f\"ann_models/model2_{highest_accuracy}.pt\")\n",
    "# model2 = torch.load(f\"ann_models/model2_{highest_accuracy}.pt\")\n",
    "model2.eval()\n",
    "y_pred_second = model2(X_test_second)\n",
    "y_pred_second_int = []\n",
    "\n",
    "for item in y_pred_second:\n",
    "    y_pred_second_int.append(round(float(item[0])))\n",
    "\n",
    "print(\"internal validation accuracy:\", np.sum(y_pred_second_int==np.array(y_test_second))/len(y_test_second))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "cb61226d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_OUT = model2(X_second)\n",
    "Y_OUTs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "67b3b230",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in Y_OUT:\n",
    "    Y_OUTs.append(round(float(item[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "f455dbc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "external validation accuracy: 0.8653846153846154\n"
     ]
    }
   ],
   "source": [
    "print(\"external validation accuracy:\", np.sum(Y_OUTs==np.array(y_second))/len(y_second))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "632f701a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zero_length 22\n",
      "external validation recall: 0.8636363636363636\n"
     ]
    }
   ],
   "source": [
    "print(\"external validation recall:\", recall(Y_OUTs, y_second))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "2f01d5d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zero_length 22\n",
      "external validation auc: 0.8636363636363636\n"
     ]
    }
   ],
   "source": [
    "fpr2, tpr2, thersholds2 = roc_curve(y_second, Y_OUTs, pos_label=1)\n",
    "roc_auc2 = auc(fpr2, tpr2)\n",
    "print(\"external validation auc:\", recall(Y_OUTs, y_second))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfbaaf3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
